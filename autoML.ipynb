{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML or Automated Machine Learning is a concept which describes the process of automating standard machine learning phases. By automating the learning phases we are trying to find the optimum neural network hyperparameters and parameters that result in model's higher accuracy and better generalization. Hence, the automated ML solutions are testing different sets of parameters and models in order to find the one that performs the best for a given task. The autoML is said to perform Neural Network Architecture Search (NAS). Automation can cover input data processing and preparation, data types identification, feature engineering, selection and extraction, hyperparameter optimization, model selection, autoML analysis and visualisation. The autoML is available as open source platform or as cloud enabled paid service.\n",
    "\n",
    "## Cloud based AutoML\n",
    "A cloud based autoML enable us to perform a complete end-to-end machine learning process in the cloud. With clearly defined problem statement we are able to do data collection and aggregation, data labeling and annotating, data splitting followed by data training, validation and evaluation. \n",
    "\n",
    "<img src=\"./images/automl.jpg\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "The end result is an optimized machine learnig model capable of detecting objects (in our case this will be a flat tv/computer screen in the image). The cloud based AutoML made available this model in three different formats. The first, tfLite was accesible for export and download to be used within the edgeAI or mobile environment, the second tf.js was available for implementation and integration into web based applications, and the third option offered was a pb (protobuf) file for implementaion in containerized environments.\n",
    "\n",
    "\n",
    "<img src=\"./images/automlexample.jpg\"/>\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML in local environment\n",
    "AutomML based machine learning is also available for installation within a local environment. We will be dealing with the one based on Keras and developed by [DATA](http://faculty.cs.tamu.edu/xiahu/index.html) Lab at Texas A&M University. AutoKeras was developed with the goal to democritize AI and make machine learning more accessible for everyone. AutoKeras is an open-source alternative to cloud based AutoML solutions.\n",
    "\n",
    "<img src=\"./images/autokeras.svg\" width=\"320\"/>\n",
    "<br><br><br>\n",
    "AutoKeras supports Image Classifiation, Image Regression, Text Classification, Text Regression, Structured Data Classification, Structured Data Regression. Moreover, the Time Series Forecasting, Object Detection and Image Segmentation machine learnig models are expected to become a part of autokeras autoML libraries.\n",
    "<br><br>\n",
    "\n",
    "More details about AutoKeras can be found in a [research paper](https://dl.acm.org/doi/pdf/10.1145/3292500.3330648) by Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. Authors give an overview of the field and explain the architecture and building blocks of automated machine learning. Figure below depicts the AutoKeras system overview.\n",
    "<br><br>\n",
    "\n",
    "<img src=\"./images/autokerasoverview.jpg\" />\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autokeras Installation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of AutoKeras can be performed using pip package manager. However, before installing it the following requirements need to be fulfilled: Python3, [Tensorflow >= 2.3.0](https://www.tensorflow.org/install/pip), [GPU installation](https://www.tensorflow.org/install/gpu) is optional but recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pip install tensorflow\n",
    "pip install git+https://github.com/keras-team/keras-tuner.git\n",
    "pip install autokeras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "# check python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# check tensorflow version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using AutoKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "\n",
    "The MNIST database contains standard handwritten digits that have been widely used for training and testing of machine learning algorithms. It has a training set of 60,000 images and a test set of 10,000 images with each image being 28 x 28 pixels. This set is easy to use visualize and train on any computer. The MNIST dataset is usually provided and downloadable as part of the Keras library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download MNIST data and split into train and test sets\n",
    "# mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion_MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cifar10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  [y for x in y_train for y in x]\n",
    "y_test = [y for x in y_test for y in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:100]\n",
    "y_test = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training set is: (10000, 28, 28)\n",
      "The shape of test set is: (100, 28, 28)\n",
      "\n",
      "The shape of label training set is: (10000,)\n",
      "The shape of label test set is: (100,)\n",
      "\n",
      "The size of training set is: 7656.25 kB\n",
      "The size of test set is: 76.5625 kB\n",
      "\n",
      "The size of label training set is: 9.765625 kB\n",
      "The size of label test set is: 0.09765625 kB\n",
      "\n",
      "The training set is: 3 - dimensional array\n",
      "The test set is: 3 - dimensional array\n",
      "\n",
      "The label training set is: 1 - dimensional array\n",
      "The label test set is: 1 - dimensional array\n",
      "\n",
      "The label training set type is: uint8\n",
      "The label test set type is: uint8\n",
      "\n",
      "The training set type is: uint8\n",
      "The test set type is: uint8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the shape, size and dtype of loaded dataset\n",
    "print(f'The shape of training set is: {X_train.shape}')\n",
    "print(f'The shape of test set is: {X_test.shape}\\n')\n",
    "\n",
    "print(f'The shape of label training set is: {y_train.shape}')\n",
    "print(f'The shape of label test set is: {y_test.shape}\\n')\n",
    "\n",
    "print(f'The size of training set is: {X_train.size/1024} kB')\n",
    "print(f'The size of test set is: {X_test.size/1024} kB\\n')\n",
    "\n",
    "print(f'The size of label training set is: {y_train.size/1024} kB')\n",
    "print(f'The size of label test set is: {y_test.size/1024} kB\\n')\n",
    "\n",
    "print(f'The training set is: {X_train.ndim} - dimensional array')\n",
    "print(f'The test set is: {X_test.ndim} - dimensional array\\n')\n",
    "\n",
    "print(f'The label training set is: {y_train.ndim} - dimensional array')\n",
    "print(f'The label test set is: {y_test.ndim} - dimensional array\\n')\n",
    "\n",
    "print(f'The label training set type is: {X_train.dtype}')\n",
    "print(f'The label test set type is: {X_test.dtype}\\n')\n",
    "\n",
    "print(f'The training set type is: {y_train.dtype}')\n",
    "print(f'The test set type is: {y_test.dtype}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  79 237  37\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  41 243 239  39\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  18 207 254 140   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 143 254 254   5   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 112 251 249 228   4   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  41 228 254 202   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   5 214 254 254 147   0   0  12\n",
      "   14   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  99 254 254 181   0   0  79 233\n",
      "  244 135   7   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  22 242 254 225  40   0   3 250 254\n",
      "  254 254 157   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  91 254 251  70   0   0 163 254 178\n",
      "  224 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  33 221 254 198   0   0  83 244 254  62\n",
      "  135 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  84 254 254  33   0   0 156 254 153   2\n",
      "   51 254 237  14   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  84 254 254  31   0  41 231 254  30   0\n",
      "  135 254 247  54   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  84 254 254  31   0 110 254 254   5   0\n",
      "  148 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  84 254 254  63   0 110 254 254   5  28\n",
      "  244 254 234   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  84 254 255 187   8 110 254 254 128 213\n",
      "  254 248  87   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  62 249 254 254 200 207 254 254 254 254\n",
      "  254 148   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  99 254 254 254 254 254 254 254 254\n",
      "  219  11   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  10 123 248 254 254 254 254 204  94\n",
      "    6   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  47 176 210 150 150  22   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#chek the dataset image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "randomSample=random.choice(X_train)\n",
    "print(randomSample)\n",
    "plt.imshow(randomSample, cmap=\"gray\")\n",
    "plt.plot()\n",
    "print(X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AxesSubplot(0.125,0.125;0.110714x0.755)\n",
      "1 AxesSubplot(0.257857,0.125;0.110714x0.755)\n",
      "2 AxesSubplot(0.390714,0.125;0.110714x0.755)\n",
      "3 AxesSubplot(0.523571,0.125;0.110714x0.755)\n",
      "4 AxesSubplot(0.656429,0.125;0.110714x0.755)\n",
      "5 AxesSubplot(0.789286,0.125;0.110714x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes=plt.subplots(nrows=1,ncols=6, figsize=(15,10))\n",
    "for i,ax in enumerate(axes):\n",
    "    print(i, ax)\n",
    "    ax.imshow(X_train[i], cmap=\"gray\")\n",
    "    ax.axis()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classifier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autokeras.tasks.image.ImageClassifier'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the image classifier. Try only 1 model architecture and increase accordingly.\n",
    "clf = ak.ImageClassifier(max_trials=1, overwrite = True)  \n",
    "print(type(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "### Data formats/types\n",
    "In terms of data formats and classification label types Autokeras accepts image data with/without channel dimension and for classification labels both plain labels or one hot encoded labels are accepted. With standard keras modeling/training we transformed both images and labels. Recall that the *y_train and y_test data* were not split into 10 distinct class labels, but represented as a single array with the class values. Thus, we needed to preprocess class labels by converting the 1 - dimensional numpy array to 10 - dimensional array. The same applies to normalization/standardization of input data. Hence, in the case of autoML we will skip data preprocessing as it was the case with standard ML model. \n",
    "\n",
    "tf.data.Dataset format is also supported as input data format:\n",
    "```python\n",
    "train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,)))\n",
    "test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 11s]\n",
      "val_loss: 0.19520288705825806\n",
      "\n",
      "Best val_loss So Far: 0.19520288705825806\n",
      "Total elapsed time: 00h 00m 11s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 0.7082 - accuracy: 0.7725\n",
      "INFO:tensorflow:Assets written to: .\\image_classifier\\best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Feed the image classifier with training data.\n",
    "training = clf.fit(X_train, y_train, epochs=1, validation_split=0.20)  # Change no of epochs to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1491 - accuracy: 0.9500\n",
      "Test loss: 0.14911793172359467\n",
      "Test accuracy: 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "score = clf.evaluate(X_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "####  Change the size of input data and see what happens?\n",
    "\n",
    "####  Change the max_trials parameters and see what happens?\n",
    "\n",
    "####  Change the value of epochs parameters and see what happens with the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModel building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ImageBlock(\n",
    "    # Only search ResNet architectures.\n",
    "    block_type=\"resnet\",\n",
    "    # Normalize the dataset.\n",
    "    normalize=True,\n",
    "    # Do not do data augmentation.\n",
    "    augment=False,\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")\n",
    "clf.fit(x_train, y_train, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.Normalization()(input_node)\n",
    "output_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\n",
    "output_node = ak.ResNetBlock(version=\"v2\")(output_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n",
    ")\n",
    "clf.fit(x_train, y_train, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as a Keras Model.\n",
    "model = clf.export_model()\n",
    "model.save(\"model_autokeras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "cast_to_float32 (CastToFloat (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "expand_last_dim (ExpandLastD (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "normalization (Normalization (None, 28, 28, 1)         3         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                92170     \n",
      "_________________________________________________________________\n",
      "classification_head_1 (Softm (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 110,989\n",
      "Trainable params: 110,986\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['7']\n",
      " ['2']\n",
      " ['1']\n",
      " ['0']\n",
      " ['4']\n",
      " ['1']\n",
      " ['4']\n",
      " ['9']\n",
      " ['6']\n",
      " ['9']]\n"
     ]
    }
   ],
   "source": [
    "predicted_y = clf.predict(X_test)\n",
    "print(predicted_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [['4']]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take an image from a test set and check the model's prediciton\n",
    "randomSample=random.choice(X_test)\n",
    "plt.imshow(randomSample, cmap=\"gray\")\n",
    "plt.plot()\n",
    "randomSample=randomSample.reshape(1,28,28)\n",
    "predicted_y = clf.predict(randomSample)\n",
    "print('\\n',predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cloud based cognitive services\n",
    "* Microsoft's Video Indexer (https://www.videoindexer.ai/)\n",
    "* Google's Video AI (https://cloud.google.com/video-intelligence)\n",
    "* Amazon's SageMaker (https://aws.amazon.com/sagemaker/?nc2=h_ql_prod_ml_sm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example how image dataset was created within a cloud environment to detect objects in images. In this example GCP - [Google Cloud Platform Vision module](https://console.cloud.google.com/vision/datasets?project=tvdetection01) was used to upload, store, create, label, train, validate and export ML model to mobile, web or containerized platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Additional information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who want to find out more the achievements in the field of Computer Vision and Image Processing please check the sources listed below:\n",
    " \n",
    "\n",
    "Check the following conferences/journals:\n",
    "\n",
    "* CVPR: IEEE Conference on Computer Vision and Pattern Recognition\n",
    "* JVIR: Journal of Visual Communication and Image Representation\n",
    "* ICCV: International Conference on Computer Vision\n",
    "* ECCV: European Conference on Computer Vision\n",
    "* NIPS: Neural Information Processing Systems\n",
    "* ICML: International Conference on Machine Learning\n",
    "\n",
    " \n",
    "\n",
    "web sites:\n",
    "\n",
    "* https://www.nvidia.com/en-us/research/computer-vision/\n",
    "\n",
    "* https://arxiv.org/\n",
    "\n",
    "* https://cs.stanford.edu/people/karpathy/deepvideo/\n",
    "\n",
    "* https://cloud.google.com/vision/\n",
    "\n",
    "* https://www.microsoft.com/en-us/research/research-area/computer-vision/\n",
    "\n",
    "* http://vision.stanford.edu/\n",
    "\n",
    "* https://www.research.ibm.com/artificial-intelligence/computer-vision/\n",
    "\n",
    " \n",
    "\n",
    "and books:\n",
    "\n",
    "* CV: Algorithms and Applications, Richard Szeliski\n",
    "\n",
    "* CV: Models, Learning, and Inference, Simon Prince\n",
    "\n",
    "* CV: A Modern Approach, D. A. Forsyth, J. P. Ponce\n",
    "\n",
    "* Machine Vision, R. Jain, R. Kasturi, B. \n",
    "\n",
    "* Practical Computer Vision With SimpleCV, Kurt DeMaagd, Anthony Oliver, Nathan Oostendorp, and Katherine Scott\n",
    "\n",
    "* Digital Image Processing, R. C. Gonzalez, R. E. Woods.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Follow the latest work/posts/blogs written by top researchers in the field such as Alex Krizhevsky, Andrew Ng,  Andrej Karpathy, Geoffrey E. Hinton, Ilya Sutskever, etc.\n",
    "\n",
    " \n",
    "Check out data competitions such as Kaggle competition related to CV and DIP - https://www.kaggle.com/competitions or DrivenData competitions related to CV and DIP - https://www.drivendata.org/competitions/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
