{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Lab - Working with visual datasets\n",
    "\n",
    "The goal of this lab is to introduce you to visual datasets collection and preprocessing before you start building and deploying your computer vision model. \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/VisualDataML.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ImportanceBigData.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ModelML.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DeepLearning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/MLCategories.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. \n",
    "<img src=\"https://keras.io/img/keras-logo-small.jpg\" width=\"140\">\n",
    "\n",
    "Keras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as tf.keras. Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is maintaining the Keras fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).\n",
    "\n",
    "<img src=\"./images/KerasStack.png\">\n",
    "\n",
    "\n",
    "## Keras backend\n",
    "Keras relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n",
    "\n",
    "At this time, Keras has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend.\n",
    "\n",
    "* [TensorFlow](https://www.tensorflow.org/lite) is an open-source symbolic tensor manipulation framework developed by Google.\n",
    "* [Theano](http://deeplearning.net/software/theano/) is an open-source symbolic tensor manipulation framework developed by LISA Lab at Université de Montréal.Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. The latest release of Theano was on 2017/11/15. \n",
    "* [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/) is an open-source toolkit for deep learning developed by Microsoft. It describes neural networks as a series of computational steps via a directed graph. CNTK allows the user to easily realize and combine popular model types such as feed-forward DNNs, convolutional neural networks (CNNs) and recurrent neural networks (RNNs/LSTMs). CNTK implements stochastic gradient descent (SGD, error backpropagation) learning with automatic differentiation and parallelization across multiple GPUs and servers.<br><br><br><br>\n",
    "\n",
    "\n",
    "## Keras popularity\n",
    "\n",
    "<img src=\"./images/KerasPop01.png\" width=\"600\">\n",
    "<br><br>\n",
    "<img src=\"./images/KerasPop02.png\" width=\"600\">\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Keras Ecosystem\n",
    "\n",
    "* ### [Keras Tuner](https://keras-team.github.io/keras-tuner/)\n",
    "scalable hyperparameter search framework\n",
    "* ### [AutoKeras](https://autokeras.com/)\n",
    "widely accessible and easy to learn and use machine learning AutoML system based on Keras\n",
    "* ### [TensorFlow Cloud](https://github.com/tensorflow/cloud)\n",
    "set of utilities for running large-scale Keras training jobs on Google Cloud Platform\n",
    "* ### [TensorFlow.js](https://www.tensorflow.org/js)\n",
    "used for running TF models in the browser or on a Node.js server\n",
    "* ### [TensorFlow Lite](https://www.tensorflow.org/lite) \n",
    "open source deep learning framework for deploying ML models on mobile and IoT devices\n",
    "* ### [Model optimization toolkit](https://www.tensorflow.org/model_optimization)\n",
    "toolkit is used to make ML models faster and more memory and power efficient\n",
    "* ### [TFX integration](https://www.tensorflow.org/tfx) \n",
    "ML platform for deploying and maintaining production machine learning pipelines\n",
    "\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability and Compatibility\n",
    "Keras also supports the [ONNX](https://onnx.ai/) format. \n",
    "\n",
    "<img src=\"https://onnx.ai/assets/mlogo.png\" width=\"140\">\n",
    "\n",
    "ONNX is a open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. ONNX is developed and supported by a community of partners.\n",
    "\n",
    "<img src=\"./images/ONNXCommunity.png\">\n",
    "\n",
    "The keras2onnx model converter enables users to convert Keras models into the ONNX model format. Initially, the Keras converter was developed in the project onnxmltools. To support more kinds of Keras models and reduce the complexity of mixing multiple converters, keras2onnx was created to convert the Keras model only.\n",
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end ML model requirements\n",
    "\n",
    "#### 1. Acquiring/Creating a dataset \n",
    "#### 2. Preparing a dataset\n",
    "#### 3. Dataset split to train and test data\n",
    "#### 4. Training the ML model\n",
    "#### 5. The ML model validation\n",
    "#### 6. ML deployment\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [CIFAR-10/100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset\n",
    "Dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "<img src=\"./images/CIFAR.png\" width=\"500\">\n",
    "\n",
    "### 2. [MNIST](http://yann.lecun.com/exdb/mnist/) Dataset\n",
    "The MNIST database has a training set of 60,000 examples, and a test set of 10,000 examples. \n",
    "<img src=\"./images/MNIST.png\" width=\"500\">\n",
    "\n",
    "### 3. IMDB-Wiki Dataset\n",
    "Dataset has 520k images - gender and age prediction.\n",
    "<img src=\"./images/IMDB.png\" width=\"500\">\n",
    "\n",
    "### 4. [ImageNet](http://www.image-net.org/challenges/LSVRC/) dataset\n",
    "Evaluation of object detection and image classification algorithms.\n",
    "<img src=\"./images/imagenet.jpeg\" width=\"500\">\n",
    "\n",
    "### 5. Places2 Database dataset\n",
    "Dateaset has more than 10 million images and over 400 scenes. It is used for scene classification and scene parsing.\n",
    "<img src=\"./images/Places.png\" width=\"500\">\n",
    "\n",
    "### 6. [COCO](http://cocodataset.org/#external) dataset\n",
    "A large-scale object detection, segmentation, and captioning dataset with several features including Object segmentation, Recognition in context, Superpixel stuff segmentation, 330K images (>200K labeled), 1.5 million object instances, 80 object categories, 91 stuff categories, 5 captions per image, 250,000 people with keypoints.\n",
    "<img src=\"./images/COCO.png\" width=\"500\">\n",
    "\n",
    "### 7. [Kaggle Datasets Collection](https://www.kaggle.com/datasets) \n",
    "Kaggle enables you to search among 36924 datasets. \n",
    "<img src=\"./images/KaggleData.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collecting remote dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collecting\n",
    "Neural networks process vectorized & standardized representations (e.g. images are read and decoded into integer tensors, converted to floating point dtypes and normalized to range between 0 and 1. In case of Keras the images can be manipulated as numpy arrays (e.g. skimage), as TensorFlow dataset objects or batches of data yielded by Python generators (e.g. keras.utils.Sequence class)\n",
    "\n",
    "The following Keras commands turn raw data on disk into a Dataset:\n",
    "\n",
    "* tf.keras.preprocessing.image_dataset_from_directory turns image files sorted into class-specific folders into a labeled dataset of image tensors.\n",
    "* tf.keras.preprocessing.text_dataset_from_directory does the same for text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tensorflow in virt env requires the following inst command \n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -q kagglecatsanddogs_3367a.zip\n",
    "import shutil\n",
    "#shutil.unpack_archive('kagglecatsanddogs_3367a.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a PetImages folder which contain two subfolders, Cat and Dog. Each subfolder contains image files for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'PetImages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5a0180373eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PetImages'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'PetImages'"
     ]
    }
   ],
   "source": [
    "os.listdir('PetImages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and remove corrupted images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the downloaded dog/cat dataset contains corrupted images and remove them if that is the case. Filtering and removal process is based on missing \"JFIF\" string that can be found in image header. \n",
    "*The JPEG File Interchange Format (JFIF) is an image file format standard. It defines supplementary specifications for the container format that contains the image data encoded with the JPEG algorithm. The base specifications for a JPEG container format are defined in Annex B of the JPEG standard, known as JPEG Interchange Format (JIF). JFIF builds over JIF to solve some of JIF's limitations, including unnecessary complexity, component sample registration, resolution, aspect ratio, and color space.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_skipped = 0\n",
    "for folder_name in (\"Cat\", \"Dog\"):\n",
    "    folder_path = os.path.join(\"PetImages\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare image size\n",
    "image_size = (180, 180)\n",
    "# declare batch size\n",
    "batch_size = 32\n",
    "\n",
    "# create training data set using image_dataset_from_directory function \n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# create validation data set\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PetImages\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize image data\n",
    "Visualise 25 images from a training set with labels as titles (1 is dog 0 is cat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(25):\n",
    "        ax = plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional images by using image augmentation\n",
    "Sometimes original image datasets can be expanded and filled with artifically generated images. These artificially generated images are result of image transformations performed on images taken from the original dataset. This process is called image augmentation and it is performed in order to make the dataset more complex (e.g. prevent overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(25):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(augmented_images[5].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options for image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. RandomContrast\n",
    "class RandomContrast: Adjust the contrast of an image or images by a random factor. Computes the global mean for each input channel/image and then computes the x of each pixel in the output image accordig to *(x - mean) * contrast_factor + mean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.RandomContrast(factor)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Example:\n",
    "layers.experimental.preprocessing.RandomContrast(0.7)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RandomCrop\n",
    "class RandomCrop: Randomly crop the images to target height and width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.RandomCrop(height, width)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Example:\n",
    "layers.experimental.preprocessing.RandomCrop(250, 250)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. RandomFlip\n",
    "class RandomFlip: Randomly flip each image horizontally and vertically. Values can be \"horizontal\", \"vertical\", or \"horizontal_and_vertical\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.RandomFlip( mode=HORIZONTAL_AND_VERTICAL, seed=None, name=None, **kwargs)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Example:\n",
    "layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, number_of_channels))\n",
    "layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(100, 100, 3))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RandomHeight\n",
    "class RandomHeight: Randomly vary the height of a batch of images during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.RandomHeight(factor, interpolation='bilinear', seed=None, name=None, **kwargs)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RandomRotation\n",
    "class RandomRotation: Randomly rotate each image. Values: height_factor=(0.2, 0.3) result in an output zoomed out by a random amount in the range [+20%, +30%]. height_factor=(-0.3, -0.2) result in an output zoomed in by a random amount in the range [+20%, +30%]. Values for fill_mode:'constant', 'reflect', 'wrap', 'nearest'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Example:\n",
    "layers.experimental.preprocessing.RandomZoom(height_factor, width_factor=None,fill_mode='reflect',interpolation='bilinear', seed=None, name=None, fill_value=0.0,**kwargs)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.random.random((32, 224, 224, 3))\n",
    "layer = tf.keras.layers.experimental.preprocessing.RandomZoom(.5, .2)\n",
    "out_img = layer(input_img)\n",
    "out_img.shape\n",
    "plt.imshow(out_img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. RandomZoom\n",
    "class RandomZoom: Randomly zoom each image during training.Values: height_factor=(0.2, 0.3) result in an output zoomed out by a random amount in the range [+20%, +30%]. height_factor=(-0.3, -0.2) result in an output zoomed in by a random amount in the range [+20%, +30%]. Values for fill_mode:'constant', 'reflect', 'wrap', 'nearest'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.RandomZoom(height_factor, width_factor=None, fill_mode='reflect', interpolation='bilinear', seed=None, name=None, fill_value=0.0,**kwargs)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Example:\n",
    "layers.experimental.preprocessing.RandomZoom(0.1)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Rescaling\n",
    "class Rescaling: Multiply inputs by scale and adds offset. To rescale an input in the [0, 255] range to be in the [0, 1] range, you would pass scale=1./255.\n",
    "\n",
    "To rescale an input in the [0, 255] range to be in the [-1, 1] range, you would pass scale=1./127.5, offset=-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.Rescaling(scale, offset=0.0, name=None, **kwargs)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Example:\n",
    "layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Resizing\n",
    "class Resizing: Image resizing layer. Interpolation values: bilinear, nearest, bicubic, area, lanczos3, lanczos5, gaussian, mitchellcubic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```layers.experimental.preprocessing.Resizing(height, width, interpolation='bilinear')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - DIP: Augmentation\n",
    "\n",
    "The goal of this exercise is to get familiar with image dataset preparations. This includes loading, visualizing and transforming of images.\n",
    "\n",
    "1. Read the image from the Cat/Dog image dataset and perform a horizontal RandomFlip transformation\n",
    "1. Visualize the result of performed horizontal RandomFlip transformation\n",
    "1. Read a group of images from the Cat/Dog image dataset and perform a RandomZoom transformation\n",
    "1. Visualize the result of performed horizontal RandomFlip transformation\n",
    "1. Print out the type, number of dimensions, size, shape of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Acquiring Visual Datasets via API\n",
    "The image datasets can be downloaded using cloud cognitive services such as MS Azure Bing Search API, or Google API or Amazon API for image searching. The [BING image search API](https://www.microsoft.com/en-us/bing/apis/bing-image-search-api) enables you to check out how image search works in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install using pip install azure-cognitiveservices-search-imagesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install azure-cognitiveservices-search-imagesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install azure-cognitiveservices-search-visualsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.search.imagesearch import ImageSearchClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import requests\n",
    "from requests import exceptions\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps to create a Bing Search Service resource through Azure Marketplace and get your key.\n",
    "\n",
    "1. Go to [Azure Portal](https://portal.azure.com/#home) and sign in with your Microsoft account. If you don't have a Microsoft account, click Create one.\n",
    "2. From the portal, type Bing in the search box.\n",
    "<br>\n",
    "<img src=\"./images/BingResource.png\">\n",
    "<br>\n",
    "\n",
    "3. Under Marketplace in the search results, select the Bing service you're interested in (for example, Bing Search or Bing Custom Search).\n",
    "<br>\n",
    "<img src=\"./images/BingSearch.png\">\n",
    "<br>\n",
    "4. If you have a free trial or pay account, skip to Create your Bing resource.\n",
    "\n",
    "<br>\n",
    "<img src=\"./images/BingCreate.png\">\n",
    "<br>\n",
    "\n",
    "5. On the Create a free account splash screen, click Start free.\n",
    "\n",
    "6. Next, you have the option of continuing with the free trial (click Start free again) or paying for an Azure subscription (click Or buy now). You can always start with the free trial and pay for a subscription later.\n",
    "\n",
    "7. Enter a resource name. Names may contain alphanumeric characters and dashes (-) only.\n",
    "\n",
    "8. The Subscription field could be set to Free Trial or select your appropriate subscription.\n",
    "\n",
    "9. In the Pricing tier dropdown, select Free F1 package. The other packages are for the pay model. To view package options and pricing for the pay model, click View full pricing details.\n",
    "\n",
    "10. If you have an existing resource group that you want to add this resource to, select it from the Resource group dropdown list. Otherwise, click Create new to create a resource group.\n",
    "\n",
    "11. Select a location from the Resource group location dropdown. The location is where the metadata associated with your account resides and has no impact on runtime availability.\n",
    "\n",
    "12. Click the check the box to indicate that you have read and understood the notice.\n",
    "\n",
    "The following image shows filled Create form.\n",
    "<br>\n",
    "<img src=\"./images/BingContainer.png\">\n",
    "<br>\n",
    "\n",
    "13. Click Create. This starts the deployment process, which can take several minutes.\n",
    "\n",
    "14. When the deployment process completes, click Go to resource.\n",
    "\n",
    "<br>\n",
    "<img src=\"./images/BingGoTo.png\">\n",
    "<br>\n",
    "\n",
    "15. To get your subscription key to use in API calls, click Keys and Endpoint in the left pane.\n",
    "\n",
    "<br>\n",
    "<img src=\"./images/BingKey.png\">\n",
    "<br>\n",
    "\n",
    "\n",
    "The keys are used to access your Bing resource. Do not share your keys. Only one key is necessary to make an API call. When regenerating the first key, you can use the second key for continued access to the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variables for your subscription key and search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_key = 'bcd3c4908e3a455a3a0a7f9a'\n",
    "#subscription_endpoint = 'https://api.cognitive.microsoft.com/bing/v7.0/images/search'\n",
    "subscription_endpoint = 'https://api.bing.microsoft.com/v7.0/images/search'\n",
    "\n",
    "max_images=20\n",
    "group_size=5\n",
    "search_term = \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when attempting to download images from the web both the Python programming language and the requests library have a number of\n",
    "# exceptions that can be thrown so let's build a list of them now so we can filter on them\n",
    "EXCEPTIONS = set([IOError, FileNotFoundError,exceptions.RequestException, exceptions.HTTPError, exceptions.ConnectionError, exceptions.Timeout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the search term in a variable then set the headers and search parameters\n",
    "term = search_term\n",
    "headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "params = {\"q\": term, \"offset\": 0, \"count\": group_size} #, 'maxHeight':200 , 'maxWidth':200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the search\n",
    "print(\"Searching Bing API for '{}'\".format(term))\n",
    "search = requests.get(subscription_endpoint, headers=headers, params=params)\n",
    "search.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the results from the search, including the total number of estimated results returned by the Bing API\n",
    "# totalEstimatedMatches\tThe estimated number of images that are relevant to the query. \n",
    "# Use this number along with the count and offset query parameters to page the results.\n",
    "\n",
    "results = search.json()\n",
    "estNumResults = min(results[\"totalEstimatedMatches\"], max_images)\n",
    "print(\"There are {} search results for '{}'\".format(estNumResults,term))\n",
    "\n",
    "# initialize the total number of images downloaded thus far\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the estimated number of results in `group_size` groups\n",
    "for offset in range(0, estNumResults, group_size):\n",
    "\n",
    "    # update the search parameters using the current offset, then make the request to fetch the results\n",
    "    print(\"Making request for group {}-{} of {}...\".format(offset, offset + group_size, estNumResults))\n",
    "    params[\"offset\"] = offset\n",
    "    time.sleep(2)\n",
    "    search = requests.get(subscription_endpoint, headers=headers, params=params)\n",
    "    search.raise_for_status()\n",
    "    results = search.json()\n",
    "    #print(results)\n",
    "    #print(\"Saving images for group {}-{} of {}...\".format(offset, offset + group_size, estNumResults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR=os.getcwd()\n",
    "output_dir=os.path.join(ROOT_DIR,search_term)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "print('Folder exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the results\n",
    "for v in results[\"value\"]:\n",
    "    # try to download the image\n",
    "    try:\n",
    "        # make a request to download the image\n",
    "        print(\"Image fetching: {}\".format(v[\"contentUrl\"]))\n",
    "        r = requests.get(v[\"contentUrl\"], timeout=30)\n",
    "        print(v[\"contentUrl\"])\n",
    "        \n",
    "        # build the path to the output image\n",
    "        ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
    "        \n",
    "        p = os.path.join(output_dir, \"{}{}\".format(str(total).zfill(8), ext))\n",
    "        print(p)\n",
    "        \n",
    "        # write the image to disk\n",
    "        f = open(p, \"wb\")\n",
    "        f.write(r.content)\n",
    "        f.close()\n",
    "        \n",
    "    # catch any errors that would not enable us to download the image\n",
    "    except Exception as e:\n",
    "        \n",
    "        # check to see if our exception is in our list of\n",
    "        # exceptions to check for\n",
    "        if type(e) in EXCEPTIONS:\n",
    "            print(\"[INFO] skipping: {}\".format(v[\"contentUrl\"]))\n",
    "            continue\n",
    "            \n",
    "    # try to load the image from disk\n",
    "    \n",
    "    image = cv2.imread(p)\n",
    "    \n",
    "    #cv2.imshow('web image',image)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    \n",
    "    # if the image is `None` then we could not properly load the\n",
    "    # image from disk (so it should be ignored)\n",
    "    if image is None:\n",
    "        print(\"[INFO] deleting: {}\".format(p))\n",
    "        os.remove(p)\n",
    "        continue\n",
    "    # update the counter\n",
    "    total += 1                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### 1. Create two folders with two groups of images for classification purpose\n",
    "#### 2. Increase the number of downloadable images\n",
    "#### 3. How would you change the size of images?\n",
    "#### 4. How would you change the width and height of images?\n",
    "#### 5. How would you download only PNG images?\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "[Filter query parameters - Reference for image query parameters](https://docs.microsoft.com/en-us/bing/search-apis/bing-image-search/reference/query-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets in Cloud environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example how image dataset was created within a cloud environment to detect objects in images. In this example GCP - [Google Cloud Platform Vision module](https://console.cloud.google.com/vision/datasets?project=tvdetection01) was used to upload, store, create, label, train, validate and export ML model to mobile, web or containerized platforms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab01",
   "language": "python",
   "name": "lab01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
